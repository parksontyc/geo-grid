{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent  # æ‰¾å‡ºæ ¹ç›®éŒ„ï¼šPath.cwd()æ‰¾å‡ºç¾åœ¨æ‰€åœ¨ç›®éŒ„(/run).parent(ä¸Šä¸€å±¤æ˜¯notebook).parent(å†ä¸Šå±¤ä¸€å±¤business_district_discovery)\n",
    "print(project_root)\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib as mlp\n",
    "FONT_PATH = project_root / \"ChineseFont.ttf\"\n",
    "fontManager.addfont(str(FONT_PATH))\n",
    "mlp.rcParams[\"font.family\"] = \"ChineseFont\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d772e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights import Queen, Rook, DistanceBand\n",
    "from esda.moran import Moran\n",
    "from esda.moran import Moran, Moran_Local\n",
    "from esda.getisord import G_Local\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from functools import wraps\n",
    "import logging\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "import contextily as ctx \n",
    "import seaborn as sns\n",
    "from branca.colormap import LinearColormap\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•†æ¥­å€ä¸­å¿ƒè­˜åˆ¥ç³»çµ±\n",
    "class AnalysisError(Exception):\n",
    "    \"\"\"è‡ªå®šç¾©åˆ†æéŒ¯èª¤é¡\"\"\"\n",
    "    pass\n",
    "\n",
    "def handle_analysis_error(func):\n",
    "    \"\"\"éŒ¯èª¤è™•ç†è£é£¾å™¨\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"{func.__name__} å¤±æ•—: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            raise AnalysisError(error_msg)\n",
    "    return wrapper\n",
    "\n",
    "class CommercialCenterIdentifier:\n",
    "    \"\"\"\n",
    "    æ”¹é€²ç‰ˆå•†æ¥­ä¸­å¿ƒè­˜åˆ¥ç³»çµ±\n",
    "    çµåˆå¤šç¨®ç©ºé–“åˆ†ææ–¹æ³•è­˜åˆ¥åŸå¸‚ä¸­çš„å•†æ¥­é›†èšå€åŸŸ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cell_size=500):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å•†æ¥­ä¸­å¿ƒè­˜åˆ¥ç³»çµ±\n",
    "        \n",
    "        Parameters:\n",
    "        cell_size: int, ç¶²æ ¼å¤§å°(ç±³)ï¼Œé è¨­500ç±³\n",
    "        \"\"\"\n",
    "        self.cell_size = cell_size\n",
    "        self.base_weights = {\n",
    "            'ä¾¿åˆ©å•†åº—': 0.1,\n",
    "            'è¶…ç´šå¸‚å ´': 0.3,\n",
    "            'å¤§å‹é‡è²©åº—': 1.0\n",
    "        }\n",
    "        \n",
    "        # è¨­ç½®æ—¥èªŒ\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "    @handle_analysis_error\n",
    "    def load_poi_from_csv(self, csv_path, x_col='longitude', y_col='latitude', \n",
    "                         type_col='type', crs='EPSG:4326'):\n",
    "        \"\"\"\n",
    "        å¾CSVæª”æ¡ˆè¼‰å…¥POIæ•¸æ“šä¸¦è½‰æ›ç‚ºGeoDataFrame\n",
    "        \n",
    "        Parameters:\n",
    "        csv_path: str, CSVæª”æ¡ˆè·¯å¾‘\n",
    "        x_col: str, ç¶“åº¦æ¬„ä½åç¨±\n",
    "        y_col: str, ç·¯åº¦æ¬„ä½åç¨±\n",
    "        type_col: str, é›¶å”®é¡å‹æ¬„ä½åç¨±\n",
    "        crs: str, åº§æ¨™åƒè€ƒç³»çµ±\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrame, åŒ…å«POIé»ä½çš„åœ°ç†æ•¸æ“šæ¡†æ¶\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è®€å–CSVæª”æ¡ˆ\n",
    "            logging.info(f\"æ­£åœ¨è®€å–CSVæª”æ¡ˆ: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # è³‡æ–™é©—è­‰\n",
    "            self._validate_poi_data(df, x_col, y_col, type_col)\n",
    "            \n",
    "            # è™•ç†ç¼ºå¤±å€¼\n",
    "            df = self._handle_missing_values(df, [x_col, y_col, type_col])\n",
    "            \n",
    "            # å‰µå»ºGeoDataFrame\n",
    "            geometry = [Point(xy) for xy in zip(df[x_col], df[y_col])]\n",
    "            gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n",
    "            \n",
    "            # è½‰æ›æŠ•å½±\n",
    "            gdf = gdf.to_crs('EPSG:3857')\n",
    "            \n",
    "            # æ·»åŠ å”¯ä¸€è­˜åˆ¥ç¢¼\n",
    "            gdf['poi_id'] = range(len(gdf))\n",
    "            \n",
    "            logging.info(f\"æˆåŠŸè¼‰å…¥ä¸¦è½‰æ› {len(gdf)} ç­†POIæ•¸æ“šè‡³EPSG:3857åº§æ¨™ç³»çµ±\")\n",
    "            \n",
    "            return gdf\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"è¼‰å…¥POIæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _validate_poi_data(self, df, x_col, y_col, type_col):\n",
    "        \"\"\"é©—è­‰POIæ•¸æ“šçš„æœ‰æ•ˆæ€§\"\"\"\n",
    "        # æª¢æŸ¥å¿…è¦æ¬„ä½\n",
    "        required_cols = [x_col, y_col]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {missing_cols}\")\n",
    "        \n",
    "        # é©—è­‰ç¶“ç·¯åº¦ç¯„åœ\n",
    "        if not df[x_col].between(-180, 180).all():\n",
    "            raise ValueError(\"ç¶“åº¦å€¼è¶…å‡ºæœ‰æ•ˆç¯„åœ (-180 åˆ° 180)\")\n",
    "        if not df[y_col].between(-90, 90).all():\n",
    "            raise ValueError(\"ç·¯åº¦å€¼è¶…å‡ºæœ‰æ•ˆç¯„åœ (-90 åˆ° 90)\")\n",
    "            \n",
    "        # é©—è­‰é¡å‹æ¬„ä½\n",
    "        if type_col not in df.columns:\n",
    "            logging.warning(f\"æ‰¾ä¸åˆ°é¡å‹æ¬„ä½ '{type_col}'ï¼Œå°‡è¨­ç½®ç‚º'unknown'\")\n",
    "            \n",
    "    def _handle_missing_values(self, df, columns):\n",
    "        \"\"\"è™•ç†ç¼ºå¤±å€¼\"\"\"\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # ç§»é™¤é—œéµæ¬„ä½ä¸­çš„ç¼ºå¤±å€¼\n",
    "        df = df.dropna(subset=columns)\n",
    "        \n",
    "        removed_count = initial_count - len(df)\n",
    "        if removed_count > 0:\n",
    "            logging.warning(f\"å·²ç§»é™¤ {removed_count} ç­†å«æœ‰ç¼ºå¤±å€¼çš„è¨˜éŒ„\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    @handle_analysis_error\n",
    "    def create_grid(self, admin_boundaries):\n",
    "        \"\"\"\n",
    "        å°‡ç ”ç©¶å€åŸŸåŠƒåˆ†ç‚ºè¦å‰‡ç¶²æ ¼ï¼Œä¸¦ç¢ºä¿ç¶²æ ¼èˆ‡è¡Œæ”¿å€åŸŸæ­£ç¢ºå°é½Š\n",
    "        \n",
    "        Parameters:\n",
    "        admin_boundaries: GeoDataFrameï¼Œè¡Œæ”¿å€åŸŸé‚Šç•Œ\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrameï¼ŒåŒ…å«ç¶²æ ¼çš„å¹¾ä½•è³‡è¨Š\n",
    "        \"\"\"\n",
    "        logging.info(\"é–‹å§‹å‰µå»ºç ”ç©¶å€åŸŸç¶²æ ¼...\")\n",
    "\n",
    "        # ç¢ºä¿åº§æ¨™ç³»çµ±ä¸€è‡´æ€§\n",
    "        if admin_boundaries.crs != 'EPSG:3857':\n",
    "            logging.info(\"è½‰æ›è¡Œæ”¿å€åŸŸé‚Šç•Œè‡³EPSG:3857åº§æ¨™ç³»çµ±\")\n",
    "            admin_boundaries = admin_boundaries.to_crs('EPSG:3857')\n",
    "        \n",
    "        try:\n",
    "            # è¨ˆç®—ç ”ç©¶å€åŸŸé‚Šç•Œ\n",
    "            bounds = admin_boundaries.total_bounds\n",
    "            x_min, y_min, x_max, y_max = bounds\n",
    "            \n",
    "            # èª¿æ•´ç¶²æ ¼ç¯„åœä»¥ç¢ºä¿å®Œæ•´è¦†è“‹\n",
    "            x_min = np.floor(x_min / self.cell_size) * self.cell_size\n",
    "            y_min = np.floor(y_min / self.cell_size) * self.cell_size\n",
    "            x_max = np.ceil(x_max / self.cell_size) * self.cell_size\n",
    "            y_max = np.ceil(y_max / self.cell_size) * self.cell_size\n",
    "            \n",
    "            # å‰µå»ºç¶²æ ¼åæ¨™\n",
    "            x_coords = np.arange(x_min, x_max + self.cell_size, self.cell_size)\n",
    "            y_coords = np.arange(y_min, y_max + self.cell_size, self.cell_size)\n",
    "            \n",
    "            # ç”Ÿæˆç¶²æ ¼\n",
    "            grid_cells = []\n",
    "            grid_info = []  # å„²å­˜ç¶²æ ¼è³‡è¨Š\n",
    "            \n",
    "            for i, x in enumerate(x_coords[:-1]):\n",
    "                for j, y in enumerate(y_coords[:-1]):\n",
    "                    # å‰µå»ºç¶²æ ¼å¤šé‚Šå½¢\n",
    "                    cell = Polygon([\n",
    "                        (x, y),\n",
    "                        (x + self.cell_size, y),\n",
    "                        (x + self.cell_size, y + self.cell_size),\n",
    "                        (x, y + self.cell_size)\n",
    "                    ])\n",
    "                    grid_cells.append(cell)\n",
    "                    \n",
    "                    # è¨˜éŒ„ç¶²æ ¼ç´¢å¼•\n",
    "                    grid_info.append({\n",
    "                        'grid_id': f'grid_{i}_{j}',\n",
    "                        'x_index': i,\n",
    "                        'y_index': j,\n",
    "                        'centroid_x': x + self.cell_size/2,\n",
    "                        'centroid_y': y + self.cell_size/2\n",
    "                    })\n",
    "            \n",
    "            # å‰µå»ºGeoDataFrame\n",
    "            grid_df = gpd.GeoDataFrame(\n",
    "                grid_info,\n",
    "                geometry=grid_cells,\n",
    "                crs=admin_boundaries.crs\n",
    "            )\n",
    "            \n",
    "            # èˆ‡è¡Œæ”¿å€åŸŸç›¸äº¤\n",
    "            grid_df = gpd.overlay(grid_df, admin_boundaries, how='intersection')\n",
    "            \n",
    "            # è¨ˆç®—å¯¦éš›é¢ç©\n",
    "            grid_df['area'] = grid_df.geometry.area\n",
    "            \n",
    "            # ç§»é™¤é¢ç©éå°çš„ç¶²æ ¼ï¼ˆå¯èƒ½æ˜¯é‚Šç•Œåˆ‡å‰²å¾Œçš„å°ç¢ç‰‡ï¼‰\n",
    "            min_area = (self.cell_size ** 2) * 0.1  # è¨­å®šæœ€å°é¢ç©é–¾å€¼\n",
    "            grid_df = grid_df[grid_df['area'] > min_area].copy()\n",
    "            \n",
    "            # é‡è¨­ç´¢å¼•\n",
    "            grid_df = grid_df.reset_index(drop=True)\n",
    "            \n",
    "            logging.info(f\"æˆåŠŸå‰µå»º {len(grid_df)} å€‹æœ‰æ•ˆç¶²æ ¼\")\n",
    "            logging.info(f\"ç¶²æ ¼å¤§å°: {self.cell_size}x{self.cell_size} ç±³\")\n",
    "            logging.info(f\"ç¸½è¦†è“‹é¢ç©: {grid_df['area'].sum()/1000000:.2f} å¹³æ–¹å…¬é‡Œ\")\n",
    "            \n",
    "            return grid_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"å‰µå»ºç¶²æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @handle_analysis_error\n",
    "    def calculate_kernel_density(self, poi_data, grid_df, weights=None):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ”¹é€²çš„åŠ æ¬Šæ ¸å¯†åº¦ï¼Œçµåˆè‡ªé©æ‡‰å¸¶å¯¬å’Œå°ç¶²æ ¼å„ªåŒ–\n",
    "        \n",
    "        é€™å€‹æ–¹æ³•ä½¿ç”¨å¤šå€‹æŠ€è¡“ä¾†æé«˜æ ¸å¯†åº¦ä¼°è¨ˆçš„æº–ç¢ºæ€§ï¼š\n",
    "        1. è‡ªé©æ‡‰å¸¶å¯¬ï¼šæ ¹æ“šå±€éƒ¨POIå¯†åº¦èª¿æ•´å¸¶å¯¬\n",
    "        2. æœ€å°å¸¶å¯¬ä¿è­·ï¼šé˜²æ­¢åœ¨ç¨€ç–å€åŸŸå¸¶å¯¬éå°\n",
    "        3. KDæ¨¹å„ªåŒ–ï¼šæé«˜è·é›¢è¨ˆç®—æ•ˆç‡\n",
    "        4. æ¬Šé‡èª¿æ•´ï¼šè€ƒæ…®ä¸åŒé¡å‹POIçš„é‡è¦æ€§\n",
    "        \n",
    "        Parameters:\n",
    "        poi_data: GeoDataFrameï¼ŒPOIé»ä½æ•¸æ“š\n",
    "        grid_df: GeoDataFrameï¼Œç¶²æ ¼æ•¸æ“š\n",
    "        weights: dictï¼Œä¸åŒé›¶å”®é¡å‹çš„æ¬Šé‡\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrameï¼ŒåŒ…å«æ¨™æº–åŒ–å¯†åº¦å€¼çš„ç¶²æ ¼æ•¸æ“š\n",
    "        \"\"\"\n",
    "        logging.info(\"é–‹å§‹è¨ˆç®—æ ¸å¯†åº¦...\")\n",
    "        \n",
    "        try:\n",
    "            # ç¢ºä¿åº§æ¨™ç³»çµ±ä¸€è‡´æ€§\n",
    "            if poi_data.crs != 'EPSG:3826':\n",
    "                poi_data = poi_data.to_crs('EPSG:3826')\n",
    "            if grid_df.crs != 'EPSG:3826':\n",
    "                grid_df = grid_df.to_crs('EPSG:3826')\n",
    "                \n",
    "            # æª¢æŸ¥POIæ•¸æ“š\n",
    "            if len(poi_data) == 0:\n",
    "                raise ValueError(\"POIæ•¸æ“šç‚ºç©º\")\n",
    "                \n",
    "            # è¨­ç½®æ¬Šé‡\n",
    "            if weights is None:\n",
    "                weights = self.base_weights\n",
    "            \n",
    "            # æå–POIåæ¨™\n",
    "            coords = np.column_stack((\n",
    "                poi_data.geometry.x.values,\n",
    "                poi_data.geometry.y.values\n",
    "            ))\n",
    "            \n",
    "            # è¨ˆç®—åŸºç¤åƒæ•¸\n",
    "            n = len(coords)\n",
    "            sigma_x = np.std(coords[:, 0])\n",
    "            sigma_y = np.std(coords[:, 1])\n",
    "            sigma = np.mean([sigma_x, sigma_y])\n",
    "            \n",
    "            # è¨ˆç®—åŸºç¤å¸¶å¯¬ï¼ˆæ”¹é€²çš„Silvermanæ³•å‰‡ï¼‰\n",
    "            base_bandwidth = sigma * (n ** (-1/6)) * 0.5  # æ·»åŠ èª¿æ•´ä¿‚æ•¸\n",
    "            \n",
    "            # è¨­å®šæœ€å°å¸¶å¯¬\n",
    "            min_bandwidth = 200  # æœ€å°å¸¶å¯¬200ç±³\n",
    "            base_bandwidth = max(base_bandwidth, min_bandwidth)\n",
    "            \n",
    "            # è¨ˆç®—è‡ªé©æ‡‰å¸¶å¯¬\n",
    "            adaptive_bandwidths = self._calculate_adaptive_bandwidth(coords)\n",
    "            \n",
    "            logging.info(f\"åŸºç¤å¸¶å¯¬: {base_bandwidth:.2f}\")\n",
    "            logging.info(f\"POIé»æ•¸é‡: {n}\")\n",
    "            \n",
    "            # è¨ˆç®—æ¯å€‹ç¶²æ ¼çš„åŠ æ¬Šæ ¸å¯†åº¦\n",
    "            density_values = []\n",
    "            total_weight = 0\n",
    "            \n",
    "            for idx, cell in grid_df.geometry.items():\n",
    "                center = cell.centroid\n",
    "                density = 0\n",
    "                cell_weights = 0\n",
    "                \n",
    "                # ä½¿ç”¨KDæ¨¹å„ªåŒ–è·é›¢è¨ˆç®—\n",
    "                tree = cKDTree(coords)\n",
    "                max_bandwidth = base_bandwidth * max(adaptive_bandwidths)\n",
    "                indices = tree.query_ball_point(\n",
    "                    [center.x, center.y],\n",
    "                    r=max_bandwidth * 3\n",
    "                )\n",
    "                \n",
    "                # è¨ˆç®—ç•¶å‰ç¶²æ ¼çš„å¯†åº¦\n",
    "                for i in indices:\n",
    "                    poi = poi_data.iloc[i]\n",
    "                    dist = center.distance(poi.geometry)\n",
    "                    h = max(base_bandwidth * adaptive_bandwidths[i], min_bandwidth)\n",
    "                    \n",
    "                    if dist <= h * 3:\n",
    "                        poi_weight = weights.get(poi['type'], 1.0)\n",
    "                        kernel_value = (3/(np.pi * h**2)) * (1 - (dist/h)**2)**2\n",
    "                        density += kernel_value * poi_weight\n",
    "                        cell_weights += poi_weight\n",
    "                \n",
    "                density_values.append(density)\n",
    "                total_weight += cell_weights\n",
    "                \n",
    "                # é¡¯ç¤ºé€²åº¦\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    logging.info(f\"å·²è™•ç† {idx + 1}/{len(grid_df)} å€‹ç¶²æ ¼\")\n",
    "            \n",
    "            # æ¨™æº–åŒ–å¯†åº¦å€¼\n",
    "            study_area = grid_df.geometry.area.sum()\n",
    "            if total_weight > 0:\n",
    "                density_values = np.array(density_values) * study_area / total_weight\n",
    "                \n",
    "                # ç§»é™¤éå°çš„å¯†åº¦å€¼ä»¥æ¸›å°‘é›œè¨Š\n",
    "                non_zero_densities = density_values[density_values > 0]\n",
    "                if len(non_zero_densities) > 0:\n",
    "                    min_density = np.percentile(non_zero_densities, 5)\n",
    "                    density_values[density_values < min_density] = 0\n",
    "            else:\n",
    "                raise ValueError(\"ç„¡æ³•è¨ˆç®—å¯†åº¦å€¼ï¼šæ‰€æœ‰æ¬Šé‡ä¹‹å’Œç‚ºé›¶\")\n",
    "            \n",
    "            grid_df['density'] = density_values\n",
    "            \n",
    "            # è¨ˆç®—çµ±è¨ˆæ‘˜è¦\n",
    "            valid_count = (grid_df['density'] > 0).sum()\n",
    "            logging.info(\"\\nå¯†åº¦è¨ˆç®—å®Œæˆ:\")\n",
    "            logging.info(f\"æœ‰æ•ˆç¶²æ ¼æ•¸é‡: {valid_count}\")\n",
    "            logging.info(f\"å¯†åº¦å€¼ç¯„åœ: [{np.min(density_values):.6f}, {np.max(density_values):.6f}]\")\n",
    "            logging.info(f\"å¹³å‡å¯†åº¦: {np.mean(density_values):.6f}\")\n",
    "            logging.info(f\"å¯†åº¦æ¨™æº–å·®: {np.std(density_values):.6f}\")\n",
    "            \n",
    "            return grid_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"è¨ˆç®—æ ¸å¯†åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_adaptive_bandwidth(self, points, min_points=100):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—è‡ªé©æ‡‰å¸¶å¯¬\n",
    "        \n",
    "        Parameters:\n",
    "        points: arrayï¼Œé»ä½åæ¨™\n",
    "        min_points: intï¼Œæœ€å°é„°è¿‘é»æ•¸\n",
    "        \n",
    "        Returns:\n",
    "        arrayï¼Œæ¯å€‹é»çš„è‡ªé©æ‡‰å¸¶å¯¬ä¿‚æ•¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = cKDTree(points)\n",
    "            distances = []\n",
    "            \n",
    "            for point in points:\n",
    "                # æŸ¥æ‰¾æœ€è¿‘çš„kå€‹é»\n",
    "                dist, _ = tree.query(point, min_points)\n",
    "                # ä½¿ç”¨æœ€é é»çš„è·é›¢ä½œç‚ºå±€éƒ¨å¯†åº¦æŒ‡æ¨™\n",
    "                distances.append(dist[-1])\n",
    "                \n",
    "            # æ¨™æº–åŒ–å¸¶å¯¬ä¿‚æ•¸\n",
    "            distances = np.array(distances)\n",
    "            median_dist = np.median(distances)\n",
    "            bandwidths = distances / median_dist\n",
    "            \n",
    "            # é™åˆ¶å¸¶å¯¬ç¯„åœï¼Œé¿å…æ¥µç«¯å€¼\n",
    "            bandwidths = np.clip(bandwidths, 0.1, 10.0)\n",
    "            \n",
    "            return bandwidths\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"è¨ˆç®—è‡ªé©æ‡‰å¸¶å¯¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_spatial_weights(self, data, weight_type='queen', distance_threshold=None):\n",
    "        \"\"\"\n",
    "        å‰µå»ºç©ºé–“æ¬Šé‡çŸ©é™£ï¼Œæ”¯æŒå¤šç¨®é„°è¿‘é—œä¿‚å®šç¾©\n",
    "        \n",
    "        Parameters:\n",
    "        data: GeoDataFrame, ç©ºé–“æ•¸æ“š\n",
    "        weight_type: str, æ¬Šé‡é¡å‹ ('queen', 'rook', æˆ– 'distance')\n",
    "        distance_threshold: float, è·é›¢é–¾å€¼(ç”¨æ–¼distanceæ¬Šé‡)\n",
    "        \n",
    "        Returns:\n",
    "        W, ç©ºé–“æ¬Šé‡çŸ©é™£å°è±¡\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"å‰µå»º{weight_type}é¡å‹çš„ç©ºé–“æ¬Šé‡çŸ©é™£...\")\n",
    "            \n",
    "            if weight_type == 'queen':\n",
    "                w = Queen.from_dataframe(data)\n",
    "            elif weight_type == 'rook':\n",
    "                w = Rook.from_dataframe(data)\n",
    "            elif weight_type == 'distance':\n",
    "                if distance_threshold is None:\n",
    "                    # è‡ªå‹•è¨ˆç®—åˆé©çš„è·é›¢é–¾å€¼\n",
    "                    coords = np.column_stack([\n",
    "                        data.geometry.centroid.x,\n",
    "                        data.geometry.centroid.y\n",
    "                    ])\n",
    "                    tree = cKDTree(coords)\n",
    "                    distances, _ = tree.query(coords, k=2)  # æŸ¥æ‰¾æœ€è¿‘é„°\n",
    "                    distance_threshold = np.median(distances[:, 1]) * 2\n",
    "                    \n",
    "                w = DistanceBand.from_dataframe(\n",
    "                    data,\n",
    "                    threshold=distance_threshold,\n",
    "                    alpha=-1.0,  # æ·»åŠ è·é›¢è¡°æ¸›\n",
    "                    binary=False\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æŒçš„æ¬Šé‡é¡å‹: {weight_type}\")\n",
    "                \n",
    "            # æ¨™æº–åŒ–æ¬Šé‡çŸ©é™£\n",
    "            w.transform = 'R'  # è¡Œæ¨™æº–åŒ–\n",
    "            \n",
    "            # é©—è­‰æ¬Šé‡çŸ©é™£\n",
    "            islands = w.islands\n",
    "            if len(islands) > 0:\n",
    "                logging.warning(f\"ç™¼ç¾ {len(islands)} å€‹å­¤ç«‹å–®å…ƒ\")\n",
    "                \n",
    "            logging.info(f\"ç©ºé–“æ¬Šé‡çŸ©é™£å‰µå»ºå®Œæˆ: {len(w.weights)} å€‹å–®å…ƒ\")\n",
    "            return w\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"å‰µå»ºç©ºé–“æ¬Šé‡çŸ©é™£æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @handle_analysis_error\n",
    "    def calculate_morans_i(self, grid_df, weight_type='queen'):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—Global Moran's IæŒ‡æ•¸å’ŒLocal Moran's IæŒ‡æ•¸\n",
    "        \n",
    "        Parameters:\n",
    "        grid_df: GeoDataFrameï¼ŒåŒ…å«å¯†åº¦å€¼çš„ç¶²æ ¼æ•¸æ“š\n",
    "        weight_type: strï¼Œç©ºé–“æ¬Šé‡é¡å‹\n",
    "        \n",
    "        Returns:\n",
    "        tupleï¼Œ(å…¨åŸŸMoran's Içµæœ, æ›´æ–°å¾Œçš„ç¶²æ ¼æ•¸æ“š)\n",
    "        \"\"\"\n",
    "        logging.info(\"é–‹å§‹è¨ˆç®—ç©ºé–“è‡ªç›¸é—œ...\")\n",
    "        \n",
    "        try:\n",
    "            # è³‡æ–™é©—è­‰\n",
    "            if grid_df.empty:\n",
    "                raise ValueError(\"ç¶²æ ¼æ•¸æ“šç‚ºç©º\")\n",
    "                \n",
    "            if 'density' not in grid_df.columns:\n",
    "                raise ValueError(\"ç¶²æ ¼æ•¸æ“šä¸­ç¼ºå°‘å¯†åº¦å€¼æ¬„ä½\")\n",
    "                \n",
    "            # è™•ç†æ¥µç«¯å€¼å’Œç•°å¸¸å€¼\n",
    "            density_values = grid_df['density'].values\n",
    "            q1, q3 = np.percentile(density_values, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            valid_values = density_values[\n",
    "                (density_values >= lower_bound) & \n",
    "                (density_values <= upper_bound)\n",
    "            ]\n",
    "            \n",
    "            if len(valid_values) < 2:\n",
    "                raise ValueError(\"æœ‰æ•ˆæ•¸æ“šä¸è¶³ä»¥é€²è¡Œç©ºé–“è‡ªç›¸é—œåˆ†æ\")\n",
    "                \n",
    "            # å‰µå»ºç©ºé–“æ¬Šé‡çŸ©é™£\n",
    "            w = self.create_spatial_weights(grid_df, weight_type)\n",
    "            \n",
    "            # è¨ˆç®—å…¨åŸŸMoran's I\n",
    "            global_moran = Moran(grid_df['density'], w)\n",
    "            \n",
    "            # è¨ˆç®—å±€éƒ¨Moran's I\n",
    "            local_moran = Moran_Local(\n",
    "                grid_df['density'].values,  # ç¢ºä¿ä½¿ç”¨æ•¸å€¼æ•¸çµ„\n",
    "                w,\n",
    "                permutations=999\n",
    "            )\n",
    "            \n",
    "            # æ·»åŠ å±€éƒ¨Moran's Içµæœåˆ°ç¶²æ ¼æ•¸æ“š\n",
    "            grid_df['local_moran_i'] = local_moran.Is\n",
    "            grid_df['local_moran_p'] = local_moran.p_sim\n",
    "            \n",
    "            # åˆ†é¡ç©ºé–“é—œè¯æ¨¡å¼\n",
    "            def classify_pattern(row):\n",
    "                if row['local_moran_p'] > 0.05:\n",
    "                    return 'éé¡¯è‘—'\n",
    "                z_score = (row['density'] - density_values.mean()) / density_values.std()\n",
    "                lag_z_score = (row['local_moran_i'] - local_moran.Is.mean()) / local_moran.Is.std()\n",
    "                \n",
    "                if z_score > 0 and lag_z_score > 0:\n",
    "                    return 'é«˜-é«˜èšé›†'\n",
    "                elif z_score < 0 and lag_z_score < 0:\n",
    "                    return 'ä½-ä½èšé›†'\n",
    "                elif z_score > 0 and lag_z_score < 0:\n",
    "                    return 'é«˜-ä½ç•°å¸¸'\n",
    "                else:\n",
    "                    return 'ä½-é«˜ç•°å¸¸'\n",
    "                    \n",
    "            grid_df['spatial_pattern'] = grid_df.apply(classify_pattern, axis=1)\n",
    "            \n",
    "            # è¼¸å‡ºåˆ†æçµæœ\n",
    "            logging.info(f\"\\nå…¨åŸŸMoran's Iåˆ†æçµæœ:\")\n",
    "            logging.info(f\"Moran's Iå€¼: {global_moran.I:.4f}\")\n",
    "            logging.info(f\"æœŸæœ›å€¼E(I): {global_moran.EI:.4f}\")\n",
    "            logging.info(f\"æ–¹å·®VAR(I): {global_moran.VI_norm:.4f}\")\n",
    "            logging.info(f\"Zåˆ†æ•¸: {global_moran.z_norm:.4f}\")\n",
    "            logging.info(f\"På€¼: {global_moran.p_norm:.4f}\")\n",
    "            \n",
    "            logging.info(\"\\nç©ºé–“é—œè¯æ¨¡å¼åˆ†å¸ƒ:\")\n",
    "            pattern_counts = grid_df['spatial_pattern'].value_counts()\n",
    "            for pattern, count in pattern_counts.items():\n",
    "                logging.info(f\"{pattern}: {count}å€‹ç¶²æ ¼\")\n",
    "                \n",
    "            return global_moran, grid_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"è¨ˆç®—Moran's Iæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @handle_analysis_error\n",
    "    def perform_hotspot_analysis(self, grid_df):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œç†±é»åˆ†æï¼Œä½¿ç”¨Getis-Ord G*çµ±è¨ˆé‡\n",
    "        \n",
    "        Parameters:\n",
    "        grid_df: GeoDataFrameï¼ŒåŒ…å«å¯†åº¦å€¼çš„ç¶²æ ¼æ•¸æ“š\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrameï¼ŒåŒ…å«ç†±é»åˆ†æçµæœçš„ç¶²æ ¼æ•¸æ“š\n",
    "        \"\"\"\n",
    "        logging.info(\"é–‹å§‹åŸ·è¡Œç†±é»åˆ†æ...\")\n",
    "        \n",
    "        try:\n",
    "            # å‰µå»ºç©ºé–“æ¬Šé‡çŸ©é™£\n",
    "            w = self.create_spatial_weights(grid_df)\n",
    "            \n",
    "            # è¨ˆç®—G*çµ±è¨ˆé‡\n",
    "            g_star = G_Local(grid_df['density'], w)\n",
    "            \n",
    "            # æ·»åŠ çµæœåˆ°æ•¸æ“šæ¡†\n",
    "            grid_df['g_star'] = g_star.Zs  # Zåˆ†æ•¸\n",
    "            grid_df['g_star_p'] = g_star.p_sim  # På€¼\n",
    "            \n",
    "            # æ›´ç©©å¥çš„ç†±é»åˆ†é¡é‚è¼¯\n",
    "            def classify_hotspot(row):\n",
    "                if row['g_star_p'] > 0.1:\n",
    "                    return 'éé¡¯è‘—å€åŸŸ'\n",
    "                \n",
    "                if row['g_star'] > 0:  # ç†±é»\n",
    "                    if row['g_star_p'] <= 0.01:\n",
    "                        return '99%é¡¯è‘—ç†±é»'\n",
    "                    elif row['g_star_p'] <= 0.05:\n",
    "                        return '95%é¡¯è‘—ç†±é»'\n",
    "                    else:\n",
    "                        return '90%é¡¯è‘—å€åŸŸ'\n",
    "                else:  # å†·é»\n",
    "                    if row['g_star_p'] <= 0.01:\n",
    "                        return '99%é¡¯è‘—å†·é»'\n",
    "                    elif row['g_star_p'] <= 0.05:\n",
    "                        return '95%é¡¯è‘—å†·é»'\n",
    "                    else:\n",
    "                        return '90%é¡¯è‘—å†·é»'\n",
    "            \n",
    "            grid_df['hotspot_type'] = grid_df.apply(classify_hotspot, axis=1)\n",
    "            \n",
    "            # çµ±è¨ˆçµæœä¸¦è¨˜éŒ„\n",
    "            hotspot_counts = grid_df['hotspot_type'].value_counts()\n",
    "            logging.info(\"\\nç†±é»åˆ†æçµæœçµ±è¨ˆï¼š\")\n",
    "            for category, count in hotspot_counts.items():\n",
    "                logging.info(f\"{category}: {count}å€‹ç¶²æ ¼\")\n",
    "                \n",
    "            # é©—è­‰åˆ†é¡çµæœ\n",
    "            unique_categories = grid_df['hotspot_type'].unique()\n",
    "            logging.info(f\"\\nç™¼ç¾çš„é¡åˆ¥: {unique_categories}\")\n",
    "            \n",
    "            return grid_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"åŸ·è¡Œç†±é»åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @handle_analysis_error\n",
    "    def identify_commercial_centers(self, grid_df, threshold_percentile=75, group_col=None):\n",
    "        \"\"\"\n",
    "        è­˜åˆ¥å•†æ¥­ä¸­å¿ƒ (æ”¯æ´åˆ†å€ç¨ç«‹è­˜åˆ¥)\n",
    "        \n",
    "        Parameters:\n",
    "        grid_df: GeoDataFrame, åŒ…å«å¯†åº¦å€¼å’Œç†±é»åˆ†æçµæœçš„ç¶²æ ¼æ•¸æ“š\n",
    "        threshold_percentile: int, å¯†åº¦é–¾å€¼ç™¾åˆ†ä½æ•¸\n",
    "        group_col: str, (æ–°å¢) åˆ†çµ„æ¬„ä½åç¨±ï¼Œä¾‹å¦‚ 'TOWNNAME'ã€‚è‹¥æä¾›ï¼Œå‰‡æœƒä¾è©²æ¬„ä½åˆ†å€è¨ˆç®—é–¾å€¼èˆ‡åˆ†ç´šã€‚\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrame, è­˜åˆ¥å‡ºçš„å•†æ¥­ä¸­å¿ƒ\n",
    "        \"\"\"\n",
    "        logging.info(\"\\né–‹å§‹è­˜åˆ¥å•†æ¥­ä¸­å¿ƒ...\")\n",
    "        \n",
    "        try:\n",
    "            # æº–å‚™çµæœå®¹å™¨\n",
    "            all_commercial_centers = []\n",
    "            \n",
    "            # å»ºç«‹å…¨åŸŸç©ºé–“ç´¢å¼• (ç”¨ä¾†æœå°‹é„°å±…ï¼Œå³ä½¿è·¨å€ä¹Ÿè¦èƒ½æ‰¾åˆ°)\n",
    "            spatial_index = grid_df.sindex\n",
    "            \n",
    "            # æ±ºå®šåˆ†çµ„é‚è¼¯\n",
    "            if group_col and group_col in grid_df.columns:\n",
    "                groups = grid_df[group_col].unique()\n",
    "                logging.info(f\"å°‡é‡å° {len(groups)} å€‹è¡Œæ”¿å€åˆ†åˆ¥é€²è¡Œè­˜åˆ¥\")\n",
    "            else:\n",
    "                groups = ['ALL']\n",
    "                if group_col:\n",
    "                    logging.warning(f\"æ‰¾ä¸åˆ°åˆ†çµ„æ¬„ä½ {group_col}ï¼Œå°‡é€²è¡Œå…¨åŸŸåˆ†æ\")\n",
    "\n",
    "            # é–‹å§‹è¿­ä»£ (å¦‚æœæ˜¯å…¨åŸŸåˆ†æï¼Œgroups åªæœ‰ä¸€å€‹ 'ALL')\n",
    "            for group_name in groups:\n",
    "                # 1. ç¯©é¸ç•¶å‰åˆ†æçš„ç¶²æ ¼\n",
    "                if group_name == 'ALL':\n",
    "                    current_grid = grid_df.copy()\n",
    "                else:\n",
    "                    current_grid = grid_df[grid_df[group_col] == group_name].copy()\n",
    "                \n",
    "                if len(current_grid) == 0:\n",
    "                    continue\n",
    "\n",
    "                # 2. è¨ˆç®—ã€Œè©²å€åŸŸã€çš„åœ¨åœ°åŒ–é–¾å€¼\n",
    "                density_values = current_grid['density'].values\n",
    "                density_values_nonzero = density_values[density_values > 0]\n",
    "                \n",
    "                if len(density_values_nonzero) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                threshold = np.percentile(density_values_nonzero, threshold_percentile)\n",
    "                \n",
    "                # é¸å–è©²å€åŸŸå…§ï¼šé«˜å¯†åº¦ ä¸” ç‚ºé¡¯è‘—ç†±é» çš„ç¶²æ ¼\n",
    "                potential_centers = current_grid[\n",
    "                    ((current_grid['density'] > threshold) & \n",
    "                    (current_grid['hotspot_type'].str.contains('é¡¯è‘—ç†±é»', na=False)))\n",
    "                ].copy()\n",
    "                \n",
    "                if potential_centers.empty:\n",
    "                    continue\n",
    "\n",
    "                # 3. æ‡‰ç”¨å››å€‹é¡å¤–çš„ç¯©é¸æ¢ä»¶\n",
    "                qualified_centers = []\n",
    "                neighbor_radius = 500  # æœå°‹åŠå¾‘\n",
    "                \n",
    "                for idx, center in potential_centers.iterrows():\n",
    "                    # æ‰¾å‡ºé„°è¿‘ç¶²æ ¼ (æ³¨æ„ï¼šé€™è£¡è¦åœ¨ *å…¨åŸŸ grid_df* ä¸­æ‰¾é„°å±…ï¼Œè™•ç†é‚Šç•Œå•é¡Œ)\n",
    "                    possible_matches_index = list(spatial_index.intersection(center.geometry.buffer(neighbor_radius).bounds))\n",
    "                    neighbors = grid_df.iloc[possible_matches_index]\n",
    "                    neighbors = neighbors[neighbors.geometry.intersects(center.geometry.buffer(neighbor_radius))]\n",
    "                    \n",
    "                    if len(neighbors) < 2:\n",
    "                        continue\n",
    "                        \n",
    "                    # è¨ˆç®—é„°è¿‘å€åŸŸç‰¹å¾µ\n",
    "                    neighbor_densities = neighbors['density'].values\n",
    "                    mean_density = np.mean(neighbor_densities)\n",
    "                    std_density = np.std(neighbor_densities)\n",
    "                    cv_density = std_density / mean_density if mean_density > 0 else 0\n",
    "                    \n",
    "                    # æ¢ä»¶2: æª¢æŸ¥å‘¨é‚Šç¶²æ ¼å¯†åº¦æ˜¯å¦ä¹Ÿè¼ƒé«˜ (ä½¿ç”¨ç•¶å‰å€åŸŸé–¾å€¼)\n",
    "                    if mean_density < threshold * 0.9:\n",
    "                        continue\n",
    "                        \n",
    "                    # æ¢ä»¶3: æª¢æŸ¥å¯†åº¦å·®ç•°\n",
    "                    if cv_density > 0.5:\n",
    "                        continue\n",
    "                        \n",
    "                    # æ¢ä»¶4: æª¢æŸ¥å¯†åº¦éš¨è·é›¢çš„è®ŠåŒ–\n",
    "                    distances = [center.geometry.centroid.distance(n.geometry.centroid) \n",
    "                            for _, n in neighbors.iterrows()]\n",
    "                    \n",
    "                    # é¿å…é™¤ä»¥é›¶æˆ–ç„¡è®Šç•°çš„éŒ¯èª¤\n",
    "                    if len(distances) > 1 and np.std(distances) > 0 and np.std(neighbor_densities) > 0:\n",
    "                        correlation = np.corrcoef(distances, neighbor_densities)[0, 1]\n",
    "                        if correlation > -0.3: # è¦æ±‚å¯†åº¦èˆ‡è·é›¢å‘ˆè² ç›¸é—œ\n",
    "                            continue\n",
    "                    \n",
    "                    qualified_centers.append(idx)\n",
    "                \n",
    "                # 4. æå–è©²å€åŸŸç¬¦åˆæ¢ä»¶çš„å•†æ¥­ä¸­å¿ƒ\n",
    "                # æ³¨æ„ï¼šé€™è£¡çš„ loc ä½¿ç”¨çš„æ˜¯åŸç´¢å¼•ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„\n",
    "                district_centers = potential_centers.loc[qualified_centers].copy()\n",
    "                \n",
    "                # 5. åœ¨åœ°åŒ–åˆ†ç´š (Level 1/2/3 åªè·Ÿè©²è¡Œæ”¿å€è‡ªå·±æ¯”)\n",
    "                if len(district_centers) > 0:\n",
    "                    district_centers['area'] = district_centers.geometry.area\n",
    "                    district_centers['perimeter'] = district_centers.geometry.length\n",
    "                    district_centers['compactness'] = (4 * np.pi * district_centers['area']) / \\\n",
    "                                                    (district_centers['perimeter'] ** 2)\n",
    "                    \n",
    "                    # è¨ˆç®—è©²è¡Œæ”¿å€å•†æ¥­ä¸­å¿ƒçš„çµ±è¨ˆæ•¸æ“š\n",
    "                    local_mean = district_centers['density'].mean()\n",
    "                    local_std = district_centers['density'].std()\n",
    "                    \n",
    "                    def classify_center_local(row):\n",
    "                        if row['density'] > local_mean + local_std:\n",
    "                            return 'ä¸€ç´šå•†æ¥­ä¸­å¿ƒ'\n",
    "                        elif row['density'] > local_mean:\n",
    "                            return 'äºŒç´šå•†æ¥­ä¸­å¿ƒ'\n",
    "                        else:\n",
    "                            return 'ä¸‰ç´šå•†æ¥­ä¸­å¿ƒ'\n",
    "                    \n",
    "                    district_centers['center_level'] = district_centers.apply(classify_center_local, axis=1)\n",
    "                    \n",
    "                    # æ·»åŠ  ID (æ ¼å¼: è¡Œæ”¿å€å_ID)\n",
    "                    prefix = f\"{group_name}_\" if group_name != 'ALL' else \"\"\n",
    "                    district_centers['center_id'] = [f\"{prefix}{i+1}\" for i in range(len(district_centers))]\n",
    "                    \n",
    "                    all_commercial_centers.append(district_centers)\n",
    "                    logging.info(f\"  > {group_name}: è­˜åˆ¥å‡º {len(district_centers)} å€‹ä¸­å¿ƒ\")\n",
    "\n",
    "            # 6. åˆä½µæ‰€æœ‰å€åŸŸçš„çµæœ\n",
    "            if all_commercial_centers:\n",
    "                final_centers = pd.concat(all_commercial_centers, ignore_index=True) # ä½¿ç”¨ ignore_index é‡å»ºç´¢å¼•\n",
    "                \n",
    "                # é‡æ–°è½‰æ›å› GeoDataFrame (concat å¾Œå¯èƒ½æœƒè®Šå›æ™®é€š DataFrame)\n",
    "                final_centers = gpd.GeoDataFrame(final_centers, geometry='geometry', crs=grid_df.crs)\n",
    "\n",
    "                logging.info(f\"\\næˆåŠŸè­˜åˆ¥å•†æ¥­ä¸­å¿ƒ (å…± {len(final_centers)} å€‹)ï¼š\")\n",
    "                logging.info(f\"æŒ‰ç­‰ç´šçµ±è¨ˆ (å…¨åŸŸåŒ¯ç¸½)ï¼š\")\n",
    "                logging.info(final_centers['center_level'].value_counts())\n",
    "                \n",
    "                return final_centers\n",
    "            else:\n",
    "                logging.warning(\"æœªèƒ½è­˜åˆ¥å‡ºç¬¦åˆæ¢ä»¶çš„å•†æ¥­ä¸­å¿ƒ\")\n",
    "                # å›å‚³ç©ºçš„ GeoDataFrame ä½†ä¿ç•™æ¬„ä½çµæ§‹\n",
    "                return gpd.GeoDataFrame(columns=grid_df.columns.tolist() + ['center_level', 'center_id'], \n",
    "                                      crs=grid_df.crs, geometry=[])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"è­˜åˆ¥å•†æ¥­ä¸­å¿ƒæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00686be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–ç³»çµ±\n",
    "class VisualizationManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def visualize_map(\n",
    "        self,\n",
    "        commercial_centers=None,    # æ”¹ç‚ºå¯é¸ï¼Œå› ç‚ºç•«å¯†åº¦åœ–æ™‚å¯èƒ½ä¸éœ€è¦å®ƒ\n",
    "        grid_df=None,               # æ”¹ç‚ºå¯é¸ï¼Œå› ç‚ºç•«ä¸­å¿ƒé»æ™‚å¯èƒ½ä¸éœ€è¦å®ƒ\n",
    "        admin_boundaries=None,\n",
    "        mode= 'centers',                 # æ–°å¢ï¼š'all', 'density', 'centers'\n",
    "        focus_target='centers',  # <--- æ–°å¢åƒæ•¸ï¼š'centers' (èšç„¦ç†±é») æˆ– 'boundary' (èšç„¦å…¨è¡Œæ”¿å€)\n",
    "        zoom=14,\n",
    "        margin=0.5,              # <--- å»ºè­°ï¼šèšç„¦ç†±é»æ™‚ï¼Œmargin è¨­å¤§ä¸€é» (0.5)ï¼Œè®“å‘¨åœå¤šéœ²å‡ºä¸€é»åœ°åœ–\n",
    "        basemap_style='light',  # <--- æ–°å¢åƒæ•¸ï¼š'light', 'dark', 'warm', 'detail'\n",
    "        figsize=(7, 9),\n",
    "        title=None,                 # è®“æ¨™é¡Œå¯ä»¥è‡ªè¨‚\n",
    "        save_path=None,             # æ–°å¢ï¼šç›´æ¥å­˜æª”çš„åŠŸèƒ½\n",
    "        show=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¦–è¦ºåŒ–åœ°åœ–ç”¢ç”Ÿå™¨\n",
    "        \n",
    "        Parameters:\n",
    "        mode: str, ç¹ªåœ–æ¨¡å¼\n",
    "              - 'all': ç–ŠåŠ æ‰€æœ‰åœ–å±¤ (å¯†åº¦ + å•†æ¥­ä¸­å¿ƒ)\n",
    "              - 'density': åƒ…ç¹ªè£½ç¶²æ ¼å¯†åº¦ç†±åŠ›åœ–\n",
    "              - 'centers': åƒ…ç¹ªè£½å•†æ¥­ä¸­å¿ƒåˆ†ç´šåœ–\n",
    "        \"\"\"\n",
    "        # å»ºç«‹ç•«å¸ƒ\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=150)\n",
    "        target_crs = \"EPSG:3857\"\n",
    "\n",
    "        # ==========================================\n",
    "        # 1. æ•¸æ“šæº–å‚™èˆ‡ CRS è½‰æ›\n",
    "        # ==========================================\n",
    "        # è™•ç†ç¶²æ ¼\n",
    "        if grid_df is not None and not grid_df.empty:\n",
    "            if grid_df.crs != target_crs:\n",
    "                grid_df = grid_df.to_crs(target_crs)\n",
    "        \n",
    "        # è™•ç†å•†æ¥­ä¸­å¿ƒ\n",
    "        if commercial_centers is not None and not commercial_centers.empty:\n",
    "            if commercial_centers.crs != target_crs:\n",
    "                commercial_centers = commercial_centers.to_crs(target_crs)\n",
    "\n",
    "        # è™•ç†è¡Œæ”¿å€\n",
    "        if admin_boundaries is not None:\n",
    "            if admin_boundaries.crs != target_crs:\n",
    "                admin_boundaries = admin_boundaries.to_crs(target_crs)\n",
    "\n",
    "        # ==========================================\n",
    "        # 2. æ ¹æ“š mode æ±ºå®šç¹ªè£½å…§å®¹\n",
    "        # ==========================================\n",
    "        \n",
    "        # --- æ¨¡å¼ A: ç¹ªè£½å¯†åº¦åœ– ('all' æˆ– 'density') ---\n",
    "        if mode in ['all', 'density']:\n",
    "            if grid_df is not None and 'density' in grid_df.columns:\n",
    "                grid_df.plot(\n",
    "                    column='density', \n",
    "                    ax=ax, \n",
    "                    cmap='YlOrRd', \n",
    "                    alpha=0.6, \n",
    "                    zorder=2,\n",
    "                    legend=True,  # åªæœ‰å¯†åº¦åœ–æ™‚é¡¯ç¤º Legend æ¯”è¼ƒæ¸…æ¥š\n",
    "                    legend_kwds={'label': \"å•†æ¥­å¯†åº¦å€¼\", 'orientation': \"horizontal\", 'shrink': 0.5}\n",
    "                )\n",
    "            else:\n",
    "                print(\"è­¦å‘Š: æ¨¡å¼åŒ…å«å¯†åº¦åœ–ï¼Œä½†æœªæä¾› grid_df æ•¸æ“š\")\n",
    "\n",
    "        # --- æ¨¡å¼ B: ç¹ªè£½å•†æ¥­ä¸­å¿ƒ ('all' æˆ– 'centers') ---\n",
    "        has_centers = False\n",
    "        if mode in ['all', 'centers']:\n",
    "            if commercial_centers is not None and not commercial_centers.empty:\n",
    "                # é¡è‰²è¨­å®š\n",
    "                # colors = {'ä¸€ç´šå•†æ¥­ä¸­å¿ƒ': '#FF0000', 'äºŒç´šå•†æ¥­ä¸­å¿ƒ': '#FFA500', \n",
    "                #           'ä¸‰ç´šå•†æ¥­ä¸­å¿ƒ': '#FFD700', 'æœªåˆ†ç´š': '#FF0000'}\n",
    "                # colors = {\n",
    "                #     'ä¸€ç´šå•†æ¥­ä¸­å¿ƒ': '#D67B73',  # è«è˜­è¿ªÂ·æŸ”ç²‰ç´… (Muted Pinkish Red)\n",
    "                #     'äºŒç´šå•†æ¥­ä¸­å¿ƒ': '#8EABC2',  # è«è˜­è¿ªÂ·ç©ºæ°£è— (Airy Blue)\n",
    "                #     'ä¸‰ç´šå•†æ¥­ä¸­å¿ƒ': '#9FB3A6',  # è«è˜­è¿ªÂ·ç°è±†ç¶  (Dusty Bean Green)\n",
    "                #     'æœªåˆ†ç´š': '#D67B73'\n",
    "                # }\n",
    "\n",
    "                colors = {\n",
    "                    'ä¸€ç´šå•†æ¥­ä¸­å¿ƒ': '#C94C4C',  # è«è˜­è¿ªÂ·éµé½ç´… (Deep Dusty Red)\n",
    "                    'äºŒç´šå•†æ¥­ä¸­å¿ƒ': '#536D84',  # è«è˜­è¿ªÂ·é‹¼è—è‰² (Steel Blue)\n",
    "                    'ä¸‰ç´šå•†æ¥­ä¸­å¿ƒ': '#869E83',  # è«è˜­è¿ªÂ·é¼ å°¾è‰ç¶  (Sage Green)\n",
    "                    'æœªåˆ†ç´š': '#C94C4C'       # é è¨­åŒç¬¬ä¸€ç´š\n",
    "                }\n",
    "                \n",
    "                # è£œå…¨æ¬„ä½\n",
    "                if 'center_level' not in commercial_centers.columns:\n",
    "                    commercial_centers = commercial_centers.copy()\n",
    "                    commercial_centers['center_level'] = 'æœªåˆ†ç´š'\n",
    "                \n",
    "                existing_levels = commercial_centers['center_level'].unique()\n",
    "                \n",
    "                for level in existing_levels:\n",
    "                    mask = commercial_centers['center_level'] == level\n",
    "                    if mask.any():\n",
    "                        commercial_centers[mask].plot(\n",
    "                            ax=ax,\n",
    "                            color=colors.get(level,'#C94C4C'),\n",
    "                            alpha=0.75,\n",
    "                            edgecolor='white',\n",
    "                            linewidth=0.6,\n",
    "                            label=level,\n",
    "                            zorder=3\n",
    "                        )\n",
    "                has_centers = True\n",
    "                \n",
    "                # æ·»åŠ å•†æ¥­ä¸­å¿ƒåœ–ä¾‹\n",
    "                # legend_elements = [\n",
    "                #     Patch(facecolor=colors.get(level, 'red'), alpha=0.6, label=level)\n",
    "                #     for level in sorted(existing_levels)\n",
    "                # ]\n",
    "                # === [é—œéµä¿®æ­£] åœ–ä¾‹é¡¯ç¤ºé †åºï¼š1 -> 2 -> 3 ===\n",
    "                desired_legend_order = ['ä¸€ç´šå•†æ¥­ä¸­å¿ƒ', 'äºŒç´šå•†æ¥­ä¸­å¿ƒ', 'ä¸‰ç´šå•†æ¥­ä¸­å¿ƒ']\n",
    "                existing_levels = set(commercial_centers['center_level'].unique())\n",
    "                \n",
    "                # åªå»ºç«‹å­˜åœ¨çš„åœ–ä¾‹\n",
    "                final_legend_levels = [l for l in desired_legend_order if l in existing_levels]\n",
    "                \n",
    "                legend_elements = [\n",
    "                    Patch(facecolor=colors.get(level), alpha=0.75, label=level)\n",
    "                    for level in final_legend_levels\n",
    "                ]\n",
    "\n",
    "                ax.legend(\n",
    "                    handles=legend_elements,\n",
    "                    title='å•†æ¥­ä¸­å¿ƒç­‰ç´š',\n",
    "                    loc='upper right',\n",
    "                    frameon=True,\n",
    "                    shadow=True\n",
    "                )\n",
    "            elif mode == 'centers':\n",
    "                print(\"è­¦å‘Š: æ¨¡å¼ç‚º centersï¼Œä½†æœªæä¾› commercial_centers æ•¸æ“š\")\n",
    "\n",
    "        # --- é€šç”¨: ç¹ªè£½è¡Œæ”¿å€é‚Šç•Œ (æ‰€æœ‰æ¨¡å¼éƒ½ç•«) ---\n",
    "        if admin_boundaries is not None:\n",
    "            admin_boundaries.plot(\n",
    "                ax=ax, facecolor='none', edgecolor=\"#5A5656\", \n",
    "                linewidth=0.6, linestyle='--', zorder=4\n",
    "            )\n",
    "\n",
    "        # ==========================================\n",
    "        # 3. è‡ªå‹•èª¿æ•´è¦–åœ–ç¯„åœ\n",
    "        # ==========================================\n",
    "        # æ ¹æ“šæ¨¡å¼æ±ºå®šå„ªå…ˆåƒè€ƒçš„ç¯„åœ\n",
    "        bounds = [13520000, 2870000, 13530000, 2880000] # é è¨­\n",
    "\n",
    "        # if admin_boundaries is not None:\n",
    "        #     bounds = admin_boundaries.total_bounds\n",
    "        # elif mode == 'centers' and has_centers:\n",
    "        #     bounds = commercial_centers.total_bounds\n",
    "        # elif mode == 'density' and grid_df is not None:\n",
    "        #     bounds = grid_df.total_bounds\n",
    "        # elif has_centers: # fallback\n",
    "        #     bounds = commercial_centers.total_bounds\n",
    "\n",
    "        # é‚è¼¯åˆ¤æ–·ï¼šæ ¹æ“š focus_target æ±ºå®šèª°æ˜¯ä¸»è§’\n",
    "        if focus_target == 'centers' and has_centers:\n",
    "            # ç­–ç•¥ Aï¼šèšç„¦åœ¨ç†±é»ä¸Š \n",
    "            bounds = commercial_centers.total_bounds\n",
    "            print(f\"ğŸ” è¦–åœ–å·²èšç„¦æ–¼å•†æ¥­ä¸­å¿ƒç¯„åœ (margin={margin})\")\n",
    "            \n",
    "        elif focus_target == 'boundary' and admin_boundaries is not None:\n",
    "            # ç­–ç•¥ Bï¼šèšç„¦åœ¨è¡Œæ”¿å€å…¨åœ–\n",
    "            bounds = admin_boundaries.total_bounds\n",
    "            print(\"ğŸ” è¦–åœ–å·²èšç„¦æ–¼è¡Œæ”¿å€é‚Šç•Œ\")\n",
    "            \n",
    "        elif has_centers: \n",
    "            # Fallback 1\n",
    "            bounds = commercial_centers.total_bounds\n",
    "        elif admin_boundaries is not None:\n",
    "            # Fallback 2\n",
    "            bounds = admin_boundaries.total_bounds            \n",
    "\n",
    "        # è¨ˆç®—ç¯„åœèˆ‡ç•™ç™½    \n",
    "        x_min, y_min, x_max, y_max = bounds\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "\n",
    "        # å¥—ç”¨ margin (è®“ç†±é»ä¸è¦è²¼è‘—åœ–æ¡†)\n",
    "        center_x = (x_min + x_max) / 2\n",
    "        center_y = (y_min + y_max) / 2\n",
    "        \n",
    "        ax.set_xlim(center_x - width * (1 + margin) / 2, center_x + width * (1 + margin) / 2)\n",
    "        ax.set_ylim(center_y - height * (1 + margin) / 2, center_y + height * (1 + margin) / 2)\n",
    "\n",
    "        # ==========================================\n",
    "        # 4. åº•åœ–èˆ‡è¼¸å‡º\n",
    "        # ==========================================\n",
    "        # try:\n",
    "        #     ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, zoom=zoom, crs=target_crs, alpha=0.8, reset_extent=False)\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        # # è¨­å®šæ¨™é¡Œ\n",
    "        # if title:\n",
    "        #     final_title = title\n",
    "        # else:\n",
    "        #     mode_names = {'all': 'ç¶œåˆåˆ†æåœ–', 'density': 'å•†æ¥­ç†±åŠ›å¯†åº¦åœ–', 'centers': 'å•†æ¥­ä¸­å¿ƒåˆ†ç´šåœ–'}\n",
    "        #     final_title = f\"{mode_names.get(mode, 'åœ°åœ–çµæœ')}\"\n",
    "            \n",
    "        # ax.set_title(final_title, fontsize=16, pad=20)\n",
    "        # ax.axis('off')\n",
    "        \n",
    "        # plt.tight_layout()\n",
    "\n",
    "        try:\n",
    "            # æ ¹æ“šåƒæ•¸é¸æ“‡åº•åœ–ä¾†æº\n",
    "            if basemap_style == 'dark':\n",
    "                source = ctx.providers.CartoDB.DarkMatter\n",
    "            elif basemap_style == 'warm':\n",
    "                source = ctx.providers.CartoDB.Voyager\n",
    "            elif basemap_style == 'detail':\n",
    "                source = ctx.providers.OpenStreetMap.Mapnik # æœ€è©³ç´°ï¼Œä½†é¡è‰²è¼ƒé›œ\n",
    "            elif basemap_style == 'bw':\n",
    "                # æ³¨æ„ï¼šStamen æœ‰æ™‚éœ€è¦ API keyï¼Œè‹¥å¤±æ•—å¯æ”¹ç”¨ Stadia.StamenTonerLite\n",
    "                source = ctx.providers.Stamen.TonerLite \n",
    "            else:\n",
    "                # é è¨­ 'light'\n",
    "                source = ctx.providers.CartoDB.Positron\n",
    "\n",
    "            ctx.add_basemap(\n",
    "                ax, \n",
    "                source=source, \n",
    "                zoom=zoom, \n",
    "                crs=target_crs, \n",
    "                alpha=0.9, # åº•åœ–ä¸é€æ˜åº¦\n",
    "                reset_extent=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"è­¦å‘Š: ç„¡æ³•è¼‰å…¥åº•åœ– ({str(e)})\")\n",
    "        \n",
    "        # å­˜æª”åŠŸèƒ½\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"åœ–è¡¨å·²å„²å­˜è‡³: {save_path}\")\n",
    "            \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close() # å¦‚æœä¸é¡¯ç¤ºå°±é—œé–‰ï¼Œé‡‹æ”¾è¨˜æ†¶é«”\n",
    "            \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3daf73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šåƒæ•¸\n",
    "\n",
    "# åˆ†æå€åŸŸè¨­å®š\n",
    "TARGET_COUNTY = 'æ–°åŒ—å¸‚'\n",
    "TARGET_TOWN = 'æ·¡æ°´å€'\n",
    "\n",
    "cell_size = 100  # ç¶²æ ¼å¤§å°(ç±³)\n",
    "\n",
    "# ä¸åŒé¤é£²é¡å‹çš„æ¬Šé‡è¨­å®š\n",
    "weights = {\n",
    "    'æ—©é¤åº—': 0.05,\n",
    "    'éºµåº—ã€å°åƒåº—': 0.3,\n",
    "    'é¤å»³': 2,\n",
    "    'èª¿ç†é£²æ–™æ”¤è²©':0.05,\n",
    "    'ä¾¿ç•¶ã€è‡ªåŠ©é¤åº—':0.1,\n",
    "    'å’–å•¡é¤¨':2,\n",
    "    'æ‰‹æ–é£²åº—':0.5,\n",
    "    'é€£é–é€Ÿé£Ÿåº—':2,\n",
    "    'åƒåˆ°é£½é¤å»³':3.0,\n",
    "    'é¤é£Ÿæ”¤è²©':0.05,\n",
    "    'èŒ¶é¤¨':0.06,\n",
    "    'é£²é…’åº—':0.04,\n",
    "    'æœ‰å¨›æ¨‚ç¯€ç›®é¤å»³':0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41812ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === è³‡æ–™è¼‰å…¥ ===\n",
    "print(\"=== é–‹å§‹è¼‰å…¥è³‡æ–™ ===\\n\")\n",
    "\n",
    "# è®€å–æ–¹æ ¼ç¶²æ ¼è³‡æ–™\n",
    "save_path_grid = r\"C:\\labs\\geo-grid\\data\\output_grids\\grids_100m\\Grid_æ–°åŒ—å¸‚_100m.parquet\"\n",
    "grid_loaded = gpd.read_parquet(save_path_grid)\n",
    "print(f\"âœ“ ç¶²æ ¼è³‡æ–™è¼‰å…¥å®Œæˆ: {len(grid_loaded)} å€‹ç¶²æ ¼\")\n",
    "\n",
    "# è®€å–åœ°åœ–è³‡æ–™\n",
    "save_path_map = r\"C:\\labs\\geo-grid\\data\\output_grids\\grids_100m\\gdf_tm2.parquet\"\n",
    "map_loaded = gpd.read_parquet(save_path_map)\n",
    "print(f\"âœ“ åœ°åœ–è³‡æ–™è¼‰å…¥å®Œæˆ: {len(map_loaded)} ç­†è¡Œæ”¿å€\")\n",
    "\n",
    "# è®€å–é¤é£²æ¥­POIè³‡æ–™\n",
    "save_path_gdf = r\"C:\\labs\\geo-grid\\data\\output_poi\\catering_gdf.parquet\"\n",
    "catering_loaded = gpd.read_parquet(save_path_gdf)\n",
    "print(f\"âœ“ é¤é£²POIè¼‰å…¥å®Œæˆ: {len(catering_loaded)} ç­†\")\n",
    "\n",
    "# ç¯©é¸åˆ†æå€åŸŸ POI\n",
    "catering_anlyzed = catering_loaded.loc[catering_loaded['è¡Œæ”¿å€'] == TARGET_TOWN].copy()\n",
    "print(f\"âœ“ ç¯©é¸{TARGET_TOWN}POI: {len(catering_anlyzed)} ç­†\\n\")\n",
    "\n",
    "print(\"æ‰€æœ‰è³‡æ–™è¼‰å…¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# æ•¸æ“šé è™•ç† (Data Preprocessing)\n",
    "# ==========================================\n",
    "\n",
    "# 1. é–å®šåˆ†æç¯„åœ (å–å¾—ä¸‰é‡å€çš„é‚Šç•Œå¹¾ä½•)\n",
    "target_boundary = map_loaded[\n",
    "    (map_loaded['COUNTYNAME'] == TARGET_COUNTY) & \n",
    "    (map_loaded['TOWNNAME'] == TARGET_TOWN)\n",
    "].copy()\n",
    "\n",
    "# ç¢ºä¿åº§æ¨™ç³»çµ±ä¸€è‡´è½‰æ›ç‚º Web Mercator (EPSG:3857) \n",
    "# Class å…§éƒ¨çš„è·é›¢è¨ˆç®—ä¾è³´ EPSG:3857 (ä»¥å…¬å°ºç‚ºå–®ä½)\n",
    "if target_boundary.crs.to_string() != 'EPSG:3826':\n",
    "    target_boundary = target_boundary.to_crs('EPSG:3826')\n",
    "\n",
    "if catering_anlyzed.crs.to_string() != 'EPSG:3826':\n",
    "    catering_anlyzed = catering_anlyzed.to_crs('EPSG:3826')\n",
    "if grid_loaded.crs.to_string() != 'EPSG:3826':\n",
    "    grid_loaded = grid_loaded.to_crs('EPSG:3826')\n",
    "\n",
    "# 2. ç¯©é¸ç›®æ¨™å€åŸŸçš„ç¶²æ ¼ (éå¸¸é‡è¦ï¼)\n",
    "# ä½ çš„ grid_loaded æ˜¯å…¨éƒ¨åˆ†å€ï¼Œå¿…é ˆåªåˆ‡å‡ºä¸‰é‡å€çš„ç¶²æ ¼ï¼Œ\n",
    "# å¦å‰‡ Moran's I è¨ˆç®—æ™‚ï¼Œå‘¨åœå¤§é‡ç©ºç™½ç¶²æ ¼æœƒå°è‡´çµ±è¨ˆåèª¤ã€‚\n",
    "print(f\"æ­£åœ¨ç¯©é¸ {TARGET_TOWN} çš„ç¶²æ ¼...\")\n",
    "grid_analyzed = gpd.overlay(grid_loaded, target_boundary, how='intersection')\n",
    "\n",
    "# é‡æ–°è¨ˆç®—åˆ‡å‰²å¾Œçš„é¢ç©ä¸¦ç¯©é™¤éå°ç¢ç‰‡\n",
    "grid_analyzed['area'] = grid_analyzed.geometry.area\n",
    "min_area_threshold = (100 * 100) * 0.1  # å‡è¨­ç¶²æ ¼æ˜¯100mï¼Œå–10%ç‚ºé–¾å€¼\n",
    "grid_analyzed = grid_analyzed[grid_analyzed['area'] > min_area_threshold].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ“ {TARGET_TOWN} åˆ†æç¶²æ ¼æ•¸é‡: {len(grid_analyzed)}\")\n",
    "\n",
    "# ==========================================\n",
    "# åˆå§‹åŒ–èˆ‡åŸ·è¡Œåˆ†æ (Execution)\n",
    "# ==========================================\n",
    "\n",
    "# 1. åˆå§‹åŒ–åˆ†æå™¨ (è¨­å®šç¶²æ ¼å¤§å°ç‚º 100m)\n",
    "identifier = CommercialCenterIdentifier(cell_size=100)\n",
    "\n",
    "# 2. è¨­å®šé¤é£²æ¥­æ¬Šé‡\n",
    "# ç”±æ–¼ä½ çš„æ•¸æ“šæ˜¯ã€Œé¤é£²æ¥­ã€ï¼ŒåŸæœ¬çš„ä¾¿åˆ©å•†åº—/è¶…å¸‚æ¬Šé‡ä¸é©ç”¨ã€‚\n",
    "# ä½ éœ€è¦æª¢æŸ¥ catering_anlyzed ä¸­æ˜¯å¦æœ‰åˆ†é¡æ¬„ä½ã€‚\n",
    "# å‡è¨­æ²’æœ‰ç´°åˆ†ï¼Œæˆ‘å€‘çµ¦äºˆæ‰€æœ‰é¤é£²é»ä½ç›¸åŒçš„æ¬Šé‡ 1.0ï¼Œæˆ–è€…ä½ å¯ä»¥æ ¹æ“šè³‡æœ¬é¡æˆ–é¡åˆ¥è¨­å®š\n",
    "catering_weights = weights\n",
    "\n",
    "# ç‚ºäº†è®“ç¨‹å¼é †åˆ©é‹è¡Œï¼Œå¦‚æœè³‡æ–™ä¸­æ²’æœ‰ 'type' æ¬„ä½ï¼Œæˆ‘å€‘æ‰‹å‹•åŠ ä¸Šä¸€å€‹é è¨­å€¼\n",
    "if 'type' not in catering_anlyzed.columns:\n",
    "    catering_anlyzed['type'] = 'default'\n",
    "\n",
    "# 3. æ­¥é©Ÿä¸€ï¼šè¨ˆç®—æ ¸å¯†åº¦ (Kernel Density)\n",
    "print(\"\\n--- æ­¥é©Ÿ 1: è¨ˆç®—æ ¸å¯†åº¦ ---\")\n",
    "grid_with_density = identifier.calculate_kernel_density(\n",
    "    poi_data=catering_anlyzed,\n",
    "    grid_df=grid_analyzed,\n",
    "    weights=catering_weights\n",
    ")\n",
    "\n",
    "# 4. æ­¥é©ŸäºŒï¼šè¨ˆç®— Moran's I (ç©ºé–“è‡ªç›¸é—œ)\n",
    "print(\"\\n--- æ­¥é©Ÿ 2: ç©ºé–“è‡ªç›¸é—œåˆ†æ ---\")\n",
    "moran_res, grid_with_moran = identifier.calculate_morans_i(\n",
    "    grid_df=grid_with_density,\n",
    "    weight_type='queen' # ä½¿ç”¨ Queen æ¥é„°é—œä¿‚\n",
    ")\n",
    "\n",
    "# 5. æ­¥é©Ÿä¸‰ï¼šç†±é»åˆ†æ (Hotspot Analysis)\n",
    "print(\"\\n--- æ­¥é©Ÿ 3: ç†±é»åˆ†æ ---\")\n",
    "grid_with_hotspot = identifier.perform_hotspot_analysis(\n",
    "    grid_df=grid_with_moran\n",
    ")\n",
    "\n",
    "# 6. æ­¥é©Ÿå››ï¼šè­˜åˆ¥å•†æ¥­ä¸­å¿ƒ\n",
    "print(\"\\n--- æ­¥é©Ÿ 4: è­˜åˆ¥å•†æ¥­ä¸­å¿ƒ ---\")\n",
    "# threshold_percentile å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´ï¼Œä¾‹å¦‚ 80 æˆ– 85\n",
    "commercial_centers = identifier.identify_commercial_centers(\n",
    "    grid_df=grid_with_hotspot,\n",
    "    threshold_percentile=80,\n",
    "    group_col='TOWNNAME'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = VisualizationManager()\n",
    "\n",
    "visualizer.visualize_map(\n",
    "    commercial_centers=commercial_centers, # å¿…é ˆçµ¦å•†æ¥­ä¸­å¿ƒ\n",
    "    admin_boundaries=target_boundary,\n",
    "    focus_target='centers',        # <--- 'centers' (èšç„¦ç†±é») æˆ– 'boundary' (èšç„¦å…¨è¡Œæ”¿å€)\n",
    "    margin=.4,                         # <--- å¢åŠ ç•™ç™½\n",
    "    mode='centers',                # <--- æŒ‡å®šæ¨¡å¼\n",
    "    title=f\"{TARGET_TOWN} è­˜åˆ¥å‡ºçš„å•†æ¥­ä¸­å¿ƒç­‰ç´š\",\n",
    "    zoom=14,\n",
    "    basemap_style='light',             # 'light', 'dark', 'warm', 'detail'\n",
    "    # save_path=\"output_centers_map.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_map(\n",
    "    commercial_centers=commercial_centers,\n",
    "    grid_df=grid_with_hotspot,  # <--- é€™ä¸€è¡Œä¸€å®šè¦æœ‰ï¼\n",
    "    admin_boundaries=target_boundary,\n",
    "    mode='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualizer.visualize_map(\n",
    "    grid_df=grid_with_hotspot,      # å¿…é ˆçµ¦ç¶²æ ¼\n",
    "    admin_boundaries=target_boundary,\n",
    "    mode='density',                 # <--- æŒ‡å®šæ¨¡å¼\n",
    "    title=f\"{TARGET_TOWN} é¤é£²æ¥­å¯†åº¦åˆ†ä½ˆ\",\n",
    "    # save_path=\"output_density_map.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50727d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d7a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-grid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
